{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44, 47, 25, 11, 43, 24, 52, 30, 45, 75, 75, 75, 33, 25, 11, 11, 20,\n",
       "       30, 57, 25, 68, 66, 31, 66, 24, 65, 30, 25, 52, 24, 30, 25, 31, 31,\n",
       "       30, 25, 31, 66, 46, 24, 37, 30, 24, 61, 24, 52, 20, 30, 26, 73, 47,\n",
       "       25, 11, 11, 20, 30, 57, 25, 68, 66, 31, 20, 30, 66, 65, 30, 26, 73,\n",
       "       47, 25, 11, 11, 20, 30, 66, 73, 30, 66, 43, 65, 30, 76, 19, 73, 75,\n",
       "       19, 25, 20, 69, 75, 75, 81, 61, 24, 52, 20, 43, 47, 66, 73], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Making training and validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178650)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44, 47, 25, 11, 43, 24, 52, 30, 45, 75, 75, 75, 33, 25, 11, 11, 20,\n",
       "        30, 57, 25, 68, 66, 31, 66, 24, 65, 30, 25, 52, 24, 30, 25, 31, 31,\n",
       "        30, 25, 31, 66, 46, 24, 37, 30, 24, 61, 24, 52, 20, 30, 26, 73],\n",
       "       [30, 25, 68, 30, 73, 76, 43, 30, 49, 76, 66, 73, 49, 30, 43, 76, 30,\n",
       "        65, 43, 25, 20,  1, 15, 30, 25, 73, 65, 19, 24, 52, 24, 74, 30,  7,\n",
       "        73, 73, 25,  1, 30, 65, 68, 66, 31, 66, 73, 49,  1, 30, 32, 26],\n",
       "       [61, 66, 73, 69, 75, 75, 15, 67, 24, 65,  1, 30, 66, 43, 29, 65, 30,\n",
       "        65, 24, 43, 43, 31, 24, 74, 69, 30, 14, 47, 24, 30, 11, 52, 66, 17,\n",
       "        24, 30, 66, 65, 30, 68, 25, 49, 73, 66, 57, 66, 17, 24, 73, 43],\n",
       "       [73, 30, 74, 26, 52, 66, 73, 49, 30, 47, 66, 65, 30, 17, 76, 73, 61,\n",
       "        24, 52, 65, 25, 43, 66, 76, 73, 30, 19, 66, 43, 47, 30, 47, 66, 65,\n",
       "        75, 32, 52, 76, 43, 47, 24, 52, 30, 19, 25, 65, 30, 43, 47, 66],\n",
       "       [30, 66, 43, 30, 66, 65,  1, 30, 65, 66, 52, 82, 15, 30, 65, 25, 66,\n",
       "        74, 30, 43, 47, 24, 30, 76, 31, 74, 30, 68, 25, 73,  1, 30, 49, 24,\n",
       "        43, 43, 66, 73, 49, 30, 26, 11,  1, 30, 25, 73, 74, 75, 17, 52],\n",
       "       [30, 71, 43, 30, 19, 25, 65, 75, 76, 73, 31, 20, 30, 19, 47, 24, 73,\n",
       "        30, 43, 47, 24, 30, 65, 25, 68, 24, 30, 24, 61, 24, 73, 66, 73, 49,\n",
       "        30, 47, 24, 30, 17, 25, 68, 24, 30, 43, 76, 30, 43, 47, 24, 66],\n",
       "       [47, 24, 73, 30, 17, 76, 68, 24, 30, 57, 76, 52, 30, 68, 24,  1, 15,\n",
       "        30, 65, 47, 24, 30, 65, 25, 66, 74,  1, 30, 25, 73, 74, 30, 19, 24,\n",
       "        73, 43, 30, 32, 25, 17, 46, 30, 66, 73, 43, 76, 30, 43, 47, 24],\n",
       "       [37, 30, 32, 26, 43, 30, 73, 76, 19, 30, 65, 47, 24, 30, 19, 76, 26,\n",
       "        31, 74, 30, 52, 24, 25, 74, 66, 31, 20, 30, 47, 25, 61, 24, 30, 65,\n",
       "        25, 17, 52, 66, 57, 66, 17, 24, 74,  1, 30, 73, 76, 43, 30, 68],\n",
       "       [43, 30, 66, 65, 73, 29, 43, 69, 30, 14, 47, 24, 20, 29, 52, 24, 30,\n",
       "        11, 52, 76, 11, 52, 66, 24, 43, 76, 52, 65, 30, 76, 57, 30, 25, 30,\n",
       "        65, 76, 52, 43,  1, 75, 32, 26, 43, 30, 19, 24, 29, 52, 24, 30],\n",
       "       [30, 65, 25, 66, 74, 30, 43, 76, 30, 47, 24, 52, 65, 24, 31, 57,  1,\n",
       "        30, 25, 73, 74, 30, 32, 24, 49, 25, 73, 30, 25, 49, 25, 66, 73, 30,\n",
       "        57, 52, 76, 68, 30, 43, 47, 24, 30, 32, 24, 49, 66, 73, 73, 66]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/35720 Training loss: 4.4201 1.1448 sec/batch\n",
      "Epoch 1/20  Iteration 2/35720 Training loss: 4.3774 0.3671 sec/batch\n",
      "Epoch 1/20  Iteration 3/35720 Training loss: 4.2136 0.3648 sec/batch\n",
      "Epoch 1/20  Iteration 4/35720 Training loss: 4.6489 0.3609 sec/batch\n",
      "Epoch 1/20  Iteration 5/35720 Training loss: 4.6076 0.3600 sec/batch\n",
      "Epoch 1/20  Iteration 6/35720 Training loss: 4.4818 0.3601 sec/batch\n",
      "Epoch 1/20  Iteration 7/35720 Training loss: 4.3757 0.3599 sec/batch\n",
      "Epoch 1/20  Iteration 8/35720 Training loss: 4.2715 0.3599 sec/batch\n",
      "Epoch 1/20  Iteration 9/35720 Training loss: 4.1896 0.3614 sec/batch\n",
      "Epoch 1/20  Iteration 10/35720 Training loss: 4.1149 0.3610 sec/batch\n",
      "Epoch 1/20  Iteration 11/35720 Training loss: 4.0514 0.3594 sec/batch\n",
      "Epoch 1/20  Iteration 12/35720 Training loss: 4.0005 0.3595 sec/batch\n",
      "Epoch 1/20  Iteration 13/35720 Training loss: 3.9508 0.3601 sec/batch\n",
      "Epoch 1/20  Iteration 14/35720 Training loss: 3.9057 0.3652 sec/batch\n",
      "Epoch 1/20  Iteration 15/35720 Training loss: 3.8673 0.3610 sec/batch\n",
      "Epoch 1/20  Iteration 16/35720 Training loss: 3.8352 0.3603 sec/batch\n",
      "Epoch 1/20  Iteration 17/35720 Training loss: 3.8082 0.3661 sec/batch\n",
      "Epoch 1/20  Iteration 18/35720 Training loss: 3.7813 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 19/35720 Training loss: 3.7584 0.3667 sec/batch\n",
      "Epoch 1/20  Iteration 20/35720 Training loss: 3.7315 0.3593 sec/batch\n",
      "Epoch 1/20  Iteration 21/35720 Training loss: 3.7124 0.3651 sec/batch\n",
      "Epoch 1/20  Iteration 22/35720 Training loss: 3.6945 0.3610 sec/batch\n",
      "Epoch 1/20  Iteration 23/35720 Training loss: 3.6767 0.3608 sec/batch\n",
      "Epoch 1/20  Iteration 24/35720 Training loss: 3.6591 0.3597 sec/batch\n",
      "Epoch 1/20  Iteration 25/35720 Training loss: 3.6447 0.3655 sec/batch\n",
      "Epoch 1/20  Iteration 26/35720 Training loss: 3.6290 0.3595 sec/batch\n",
      "Epoch 1/20  Iteration 27/35720 Training loss: 3.6152 0.3643 sec/batch\n",
      "Epoch 1/20  Iteration 28/35720 Training loss: 3.6013 0.3649 sec/batch\n",
      "Epoch 1/20  Iteration 29/35720 Training loss: 3.5879 0.3623 sec/batch\n",
      "Epoch 1/20  Iteration 30/35720 Training loss: 3.5748 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 31/35720 Training loss: 3.5639 0.3611 sec/batch\n",
      "Epoch 1/20  Iteration 32/35720 Training loss: 3.5519 0.3595 sec/batch\n",
      "Epoch 1/20  Iteration 33/35720 Training loss: 3.5410 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 34/35720 Training loss: 3.5309 0.3614 sec/batch\n",
      "Epoch 1/20  Iteration 35/35720 Training loss: 3.5214 0.3656 sec/batch\n",
      "Epoch 1/20  Iteration 36/35720 Training loss: 3.5135 0.3606 sec/batch\n",
      "Epoch 1/20  Iteration 37/35720 Training loss: 3.5049 0.3617 sec/batch\n",
      "Epoch 1/20  Iteration 38/35720 Training loss: 3.4973 0.3628 sec/batch\n",
      "Epoch 1/20  Iteration 39/35720 Training loss: 3.4900 0.3631 sec/batch\n",
      "Epoch 1/20  Iteration 40/35720 Training loss: 3.4828 0.3672 sec/batch\n",
      "Epoch 1/20  Iteration 41/35720 Training loss: 3.4775 0.3607 sec/batch\n",
      "Epoch 1/20  Iteration 42/35720 Training loss: 3.4708 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 43/35720 Training loss: 3.4644 0.3599 sec/batch\n",
      "Epoch 1/20  Iteration 44/35720 Training loss: 3.4564 0.3625 sec/batch\n",
      "Epoch 1/20  Iteration 45/35720 Training loss: 3.4495 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 46/35720 Training loss: 3.4444 0.3698 sec/batch\n",
      "Epoch 1/20  Iteration 47/35720 Training loss: 3.4393 0.3582 sec/batch\n",
      "Epoch 1/20  Iteration 48/35720 Training loss: 3.4341 0.3617 sec/batch\n",
      "Epoch 1/20  Iteration 49/35720 Training loss: 3.4291 0.3600 sec/batch\n",
      "Epoch 1/20  Iteration 50/35720 Training loss: 3.4240 0.3597 sec/batch\n",
      "Epoch 1/20  Iteration 51/35720 Training loss: 3.4192 0.3651 sec/batch\n",
      "Epoch 1/20  Iteration 52/35720 Training loss: 3.4137 0.3604 sec/batch\n",
      "Epoch 1/20  Iteration 53/35720 Training loss: 3.4106 0.3600 sec/batch\n",
      "Epoch 1/20  Iteration 54/35720 Training loss: 3.4067 0.3605 sec/batch\n",
      "Epoch 1/20  Iteration 55/35720 Training loss: 3.4034 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 56/35720 Training loss: 3.3992 0.3653 sec/batch\n",
      "Epoch 1/20  Iteration 57/35720 Training loss: 3.3957 0.3595 sec/batch\n",
      "Epoch 1/20  Iteration 58/35720 Training loss: 3.3930 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 59/35720 Training loss: 3.3889 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 60/35720 Training loss: 3.3848 0.3641 sec/batch\n",
      "Epoch 1/20  Iteration 61/35720 Training loss: 3.3819 0.3585 sec/batch\n",
      "Epoch 1/20  Iteration 62/35720 Training loss: 3.3783 0.3586 sec/batch\n",
      "Epoch 1/20  Iteration 63/35720 Training loss: 3.3747 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 64/35720 Training loss: 3.3712 0.3641 sec/batch\n",
      "Epoch 1/20  Iteration 65/35720 Training loss: 3.3673 0.3592 sec/batch\n",
      "Epoch 1/20  Iteration 66/35720 Training loss: 3.3634 0.3593 sec/batch\n",
      "Epoch 1/20  Iteration 67/35720 Training loss: 3.3599 0.3617 sec/batch\n",
      "Epoch 1/20  Iteration 68/35720 Training loss: 3.3573 0.3601 sec/batch\n",
      "Epoch 1/20  Iteration 69/35720 Training loss: 3.3549 0.3599 sec/batch\n",
      "Epoch 1/20  Iteration 70/35720 Training loss: 3.3521 0.3601 sec/batch\n",
      "Epoch 1/20  Iteration 71/35720 Training loss: 3.3498 0.3673 sec/batch\n",
      "Epoch 1/20  Iteration 72/35720 Training loss: 3.3474 0.3604 sec/batch\n",
      "Epoch 1/20  Iteration 73/35720 Training loss: 3.3441 0.3645 sec/batch\n",
      "Epoch 1/20  Iteration 74/35720 Training loss: 3.3411 0.3654 sec/batch\n",
      "Epoch 1/20  Iteration 75/35720 Training loss: 3.3383 0.3605 sec/batch\n",
      "Epoch 1/20  Iteration 76/35720 Training loss: 3.3357 0.3597 sec/batch\n",
      "Epoch 1/20  Iteration 77/35720 Training loss: 3.3330 0.3595 sec/batch\n",
      "Epoch 1/20  Iteration 78/35720 Training loss: 3.3316 0.3617 sec/batch\n",
      "Epoch 1/20  Iteration 79/35720 Training loss: 3.3286 0.3637 sec/batch\n",
      "Epoch 1/20  Iteration 80/35720 Training loss: 3.3269 0.3618 sec/batch\n",
      "Epoch 1/20  Iteration 81/35720 Training loss: 3.3239 0.3610 sec/batch\n",
      "Epoch 1/20  Iteration 82/35720 Training loss: 3.3214 0.3600 sec/batch\n",
      "Epoch 1/20  Iteration 83/35720 Training loss: 3.3187 0.3590 sec/batch\n",
      "Epoch 1/20  Iteration 84/35720 Training loss: 3.3167 0.3591 sec/batch\n",
      "Epoch 1/20  Iteration 85/35720 Training loss: 3.3152 0.3601 sec/batch\n",
      "Epoch 1/20  Iteration 86/35720 Training loss: 3.3131 0.3595 sec/batch\n",
      "Epoch 1/20  Iteration 87/35720 Training loss: 3.3115 0.3591 sec/batch\n",
      "Epoch 1/20  Iteration 88/35720 Training loss: 3.3092 0.3615 sec/batch\n",
      "Epoch 1/20  Iteration 89/35720 Training loss: 3.3070 0.3593 sec/batch\n",
      "Epoch 1/20  Iteration 90/35720 Training loss: 3.3049 0.3596 sec/batch\n",
      "Epoch 1/20  Iteration 91/35720 Training loss: 3.3025 0.3594 sec/batch\n",
      "Epoch 1/20  Iteration 92/35720 Training loss: 3.3002 0.3597 sec/batch\n",
      "Epoch 1/20  Iteration 93/35720 Training loss: 3.2982 0.3590 sec/batch\n",
      "Epoch 1/20  Iteration 94/35720 Training loss: 3.2962 0.3595 sec/batch\n",
      "Epoch 1/20  Iteration 95/35720 Training loss: 3.2948 0.3597 sec/batch\n",
      "Epoch 1/20  Iteration 96/35720 Training loss: 3.2927 0.3602 sec/batch\n",
      "Epoch 1/20  Iteration 97/35720 Training loss: 3.2920 0.3607 sec/batch\n",
      "Epoch 1/20  Iteration 98/35720 Training loss: 3.2910 0.3666 sec/batch\n",
      "Epoch 1/20  Iteration 99/35720 Training loss: 3.2894 0.3659 sec/batch\n",
      "Epoch 1/20  Iteration 100/35720 Training loss: 3.2886 0.3659 sec/batch\n",
      "Epoch 1/20  Iteration 101/35720 Training loss: 3.2870 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 102/35720 Training loss: 3.2858 0.3606 sec/batch\n",
      "Epoch 1/20  Iteration 103/35720 Training loss: 3.2838 0.3605 sec/batch\n",
      "Epoch 1/20  Iteration 104/35720 Training loss: 3.2824 0.3615 sec/batch\n",
      "Epoch 1/20  Iteration 105/35720 Training loss: 3.2804 0.3599 sec/batch\n",
      "Epoch 1/20  Iteration 106/35720 Training loss: 3.2789 0.3599 sec/batch\n",
      "Epoch 1/20  Iteration 107/35720 Training loss: 3.2774 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 108/35720 Training loss: 3.2757 0.3614 sec/batch\n",
      "Epoch 1/20  Iteration 109/35720 Training loss: 3.2740 0.3591 sec/batch\n",
      "Epoch 1/20  Iteration 110/35720 Training loss: 3.2723 0.3703 sec/batch\n",
      "Epoch 1/20  Iteration 111/35720 Training loss: 3.2709 0.3594 sec/batch\n",
      "Epoch 1/20  Iteration 112/35720 Training loss: 3.2692 0.3601 sec/batch\n",
      "Epoch 1/20  Iteration 113/35720 Training loss: 3.2684 0.3596 sec/batch\n",
      "Epoch 1/20  Iteration 114/35720 Training loss: 3.2672 0.3610 sec/batch\n",
      "Epoch 1/20  Iteration 115/35720 Training loss: 3.2715 0.3656 sec/batch\n",
      "Epoch 1/20  Iteration 116/35720 Training loss: 3.2884 0.3594 sec/batch\n",
      "Epoch 1/20  Iteration 117/35720 Training loss: 3.3016 0.3592 sec/batch\n",
      "Epoch 1/20  Iteration 118/35720 Training loss: 3.3094 0.3599 sec/batch\n",
      "Epoch 1/20  Iteration 119/35720 Training loss: 3.3088 0.3587 sec/batch\n",
      "Epoch 1/20  Iteration 120/35720 Training loss: 3.3075 0.3589 sec/batch\n",
      "Epoch 1/20  Iteration 121/35720 Training loss: 3.3065 0.3615 sec/batch\n",
      "Epoch 1/20  Iteration 122/35720 Training loss: 3.3060 0.3579 sec/batch\n",
      "Epoch 1/20  Iteration 123/35720 Training loss: 3.3051 0.3584 sec/batch\n",
      "Epoch 1/20  Iteration 124/35720 Training loss: 3.3038 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 125/35720 Training loss: 3.3026 0.3593 sec/batch\n",
      "Epoch 1/20  Iteration 126/35720 Training loss: 3.3014 0.3575 sec/batch\n",
      "Epoch 1/20  Iteration 127/35720 Training loss: 3.3004 0.3658 sec/batch\n",
      "Epoch 1/20  Iteration 128/35720 Training loss: 3.2993 0.3646 sec/batch\n",
      "Epoch 1/20  Iteration 129/35720 Training loss: 3.2980 0.3572 sec/batch\n",
      "Epoch 1/20  Iteration 130/35720 Training loss: 3.2968 0.3570 sec/batch\n",
      "Epoch 1/20  Iteration 131/35720 Training loss: 3.2953 0.3582 sec/batch\n",
      "Epoch 1/20  Iteration 132/35720 Training loss: 3.2939 0.3582 sec/batch\n",
      "Epoch 1/20  Iteration 133/35720 Training loss: 3.2922 0.3666 sec/batch\n",
      "Epoch 1/20  Iteration 134/35720 Training loss: 3.2907 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 135/35720 Training loss: 3.2893 0.3574 sec/batch\n",
      "Epoch 1/20  Iteration 136/35720 Training loss: 3.2879 0.3576 sec/batch\n",
      "Epoch 1/20  Iteration 137/35720 Training loss: 3.2867 0.3620 sec/batch\n",
      "Epoch 1/20  Iteration 138/35720 Training loss: 3.2850 0.3602 sec/batch\n",
      "Epoch 1/20  Iteration 139/35720 Training loss: 3.2837 0.3632 sec/batch\n",
      "Epoch 1/20  Iteration 140/35720 Training loss: 3.2820 0.3573 sec/batch\n",
      "Epoch 1/20  Iteration 141/35720 Training loss: 3.2808 0.3603 sec/batch\n",
      "Epoch 1/20  Iteration 142/35720 Training loss: 3.2793 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 143/35720 Training loss: 3.2777 0.3571 sec/batch\n",
      "Epoch 1/20  Iteration 144/35720 Training loss: 3.2761 0.3608 sec/batch\n",
      "Epoch 1/20  Iteration 145/35720 Training loss: 3.2743 0.3610 sec/batch\n",
      "Epoch 1/20  Iteration 146/35720 Training loss: 3.2725 0.3619 sec/batch\n",
      "Epoch 1/20  Iteration 147/35720 Training loss: 3.2709 0.3628 sec/batch\n",
      "Epoch 1/20  Iteration 148/35720 Training loss: 3.2692 0.3581 sec/batch\n",
      "Epoch 1/20  Iteration 149/35720 Training loss: 3.2671 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 150/35720 Training loss: 3.2654 0.3575 sec/batch\n",
      "Epoch 1/20  Iteration 151/35720 Training loss: 3.2638 0.3574 sec/batch\n",
      "Epoch 1/20  Iteration 152/35720 Training loss: 3.2618 0.3575 sec/batch\n",
      "Epoch 1/20  Iteration 153/35720 Training loss: 3.2602 0.3575 sec/batch\n",
      "Epoch 1/20  Iteration 154/35720 Training loss: 3.2587 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 155/35720 Training loss: 3.2570 0.3573 sec/batch\n",
      "Epoch 1/20  Iteration 156/35720 Training loss: 3.2556 0.3588 sec/batch\n",
      "Epoch 1/20  Iteration 157/35720 Training loss: 3.2538 0.3581 sec/batch\n",
      "Epoch 1/20  Iteration 158/35720 Training loss: 3.2522 0.3606 sec/batch\n",
      "Epoch 1/20  Iteration 159/35720 Training loss: 3.2506 0.3582 sec/batch\n",
      "Epoch 1/20  Iteration 160/35720 Training loss: 3.2489 0.3579 sec/batch\n",
      "Epoch 1/20  Iteration 161/35720 Training loss: 3.2469 0.3592 sec/batch\n",
      "Epoch 1/20  Iteration 162/35720 Training loss: 3.2450 0.3624 sec/batch\n",
      "Epoch 1/20  Iteration 163/35720 Training loss: 3.2430 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 164/35720 Training loss: 3.2413 0.3622 sec/batch\n",
      "Epoch 1/20  Iteration 165/35720 Training loss: 3.2395 0.3608 sec/batch\n",
      "Epoch 1/20  Iteration 166/35720 Training loss: 3.2378 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 167/35720 Training loss: 3.2359 0.3605 sec/batch\n",
      "Epoch 1/20  Iteration 168/35720 Training loss: 3.2344 0.3628 sec/batch\n",
      "Epoch 1/20  Iteration 169/35720 Training loss: 3.2324 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 170/35720 Training loss: 3.2306 0.3619 sec/batch\n",
      "Epoch 1/20  Iteration 171/35720 Training loss: 3.2291 0.3581 sec/batch\n",
      "Epoch 1/20  Iteration 172/35720 Training loss: 3.2274 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 173/35720 Training loss: 3.2257 0.3625 sec/batch\n",
      "Epoch 1/20  Iteration 174/35720 Training loss: 3.2238 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 175/35720 Training loss: 3.2217 0.3620 sec/batch\n",
      "Epoch 1/20  Iteration 176/35720 Training loss: 3.2200 0.3576 sec/batch\n",
      "Epoch 1/20  Iteration 177/35720 Training loss: 3.2185 0.3587 sec/batch\n",
      "Epoch 1/20  Iteration 178/35720 Training loss: 3.2169 0.3602 sec/batch\n",
      "Epoch 1/20  Iteration 179/35720 Training loss: 3.2151 0.3618 sec/batch\n",
      "Epoch 1/20  Iteration 180/35720 Training loss: 3.2130 0.3581 sec/batch\n",
      "Epoch 1/20  Iteration 181/35720 Training loss: 3.2112 0.3604 sec/batch\n",
      "Epoch 1/20  Iteration 182/35720 Training loss: 3.2095 0.3618 sec/batch\n",
      "Epoch 1/20  Iteration 183/35720 Training loss: 3.2075 0.3579 sec/batch\n",
      "Epoch 1/20  Iteration 184/35720 Training loss: 3.2056 0.3631 sec/batch\n",
      "Epoch 1/20  Iteration 185/35720 Training loss: 3.2034 0.3595 sec/batch\n",
      "Epoch 1/20  Iteration 186/35720 Training loss: 3.2011 0.3595 sec/batch\n",
      "Epoch 1/20  Iteration 187/35720 Training loss: 3.1988 0.3611 sec/batch\n",
      "Epoch 1/20  Iteration 188/35720 Training loss: 3.1968 0.3610 sec/batch\n",
      "Epoch 1/20  Iteration 189/35720 Training loss: 3.1947 0.3599 sec/batch\n",
      "Epoch 1/20  Iteration 190/35720 Training loss: 3.1924 0.3619 sec/batch\n",
      "Epoch 1/20  Iteration 191/35720 Training loss: 3.1902 0.3614 sec/batch\n",
      "Epoch 1/20  Iteration 192/35720 Training loss: 3.1884 0.3571 sec/batch\n",
      "Epoch 1/20  Iteration 193/35720 Training loss: 3.1865 0.3567 sec/batch\n",
      "Epoch 1/20  Iteration 194/35720 Training loss: 3.1846 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 195/35720 Training loss: 3.1824 0.3575 sec/batch\n",
      "Epoch 1/20  Iteration 196/35720 Training loss: 3.1802 0.3614 sec/batch\n",
      "Epoch 1/20  Iteration 197/35720 Training loss: 3.1783 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 198/35720 Training loss: 3.1762 0.3602 sec/batch\n",
      "Epoch 1/20  Iteration 199/35720 Training loss: 3.1743 0.3602 sec/batch\n",
      "Epoch 1/20  Iteration 200/35720 Training loss: 3.1721 0.3617 sec/batch\n",
      "Validation loss: 2.75629 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 201/35720 Training loss: 3.1698 0.3603 sec/batch\n",
      "Epoch 1/20  Iteration 202/35720 Training loss: 3.1678 0.3572 sec/batch\n",
      "Epoch 1/20  Iteration 203/35720 Training loss: 3.1655 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 204/35720 Training loss: 3.1635 0.3606 sec/batch\n",
      "Epoch 1/20  Iteration 205/35720 Training loss: 3.1615 0.3605 sec/batch\n",
      "Epoch 1/20  Iteration 206/35720 Training loss: 3.1593 0.3586 sec/batch\n",
      "Epoch 1/20  Iteration 207/35720 Training loss: 3.1572 0.3622 sec/batch\n",
      "Epoch 1/20  Iteration 208/35720 Training loss: 3.1547 0.3596 sec/batch\n",
      "Epoch 1/20  Iteration 209/35720 Training loss: 3.1522 0.3610 sec/batch\n",
      "Epoch 1/20  Iteration 210/35720 Training loss: 3.1499 0.3601 sec/batch\n",
      "Epoch 1/20  Iteration 211/35720 Training loss: 3.1476 0.3579 sec/batch\n",
      "Epoch 1/20  Iteration 212/35720 Training loss: 3.1455 0.3616 sec/batch\n",
      "Epoch 1/20  Iteration 213/35720 Training loss: 3.1432 0.3608 sec/batch\n",
      "Epoch 1/20  Iteration 214/35720 Training loss: 3.1406 0.3576 sec/batch\n",
      "Epoch 1/20  Iteration 215/35720 Training loss: 3.1382 0.3586 sec/batch\n",
      "Epoch 1/20  Iteration 216/35720 Training loss: 3.1358 0.3674 sec/batch\n",
      "Epoch 1/20  Iteration 217/35720 Training loss: 3.1335 0.3607 sec/batch\n",
      "Epoch 1/20  Iteration 218/35720 Training loss: 3.1310 0.3599 sec/batch\n",
      "Epoch 1/20  Iteration 219/35720 Training loss: 3.1287 0.3646 sec/batch\n",
      "Epoch 1/20  Iteration 220/35720 Training loss: 3.1265 0.3580 sec/batch\n",
      "Epoch 1/20  Iteration 221/35720 Training loss: 3.1243 0.3752 sec/batch\n",
      "Epoch 1/20  Iteration 222/35720 Training loss: 3.1220 0.3601 sec/batch\n",
      "Epoch 1/20  Iteration 223/35720 Training loss: 3.1197 0.3614 sec/batch\n",
      "Epoch 1/20  Iteration 224/35720 Training loss: 3.1174 0.3576 sec/batch\n",
      "Epoch 1/20  Iteration 225/35720 Training loss: 3.1152 0.3625 sec/batch\n",
      "Epoch 1/20  Iteration 226/35720 Training loss: 3.1130 0.3579 sec/batch\n",
      "Epoch 1/20  Iteration 227/35720 Training loss: 3.1107 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 228/35720 Training loss: 3.1086 0.3584 sec/batch\n",
      "Epoch 1/20  Iteration 229/35720 Training loss: 3.1066 0.3597 sec/batch\n",
      "Epoch 1/20  Iteration 230/35720 Training loss: 3.1045 0.3612 sec/batch\n",
      "Epoch 1/20  Iteration 231/35720 Training loss: 3.1020 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 232/35720 Training loss: 3.0997 0.3582 sec/batch\n",
      "Epoch 1/20  Iteration 233/35720 Training loss: 3.0976 0.3617 sec/batch\n",
      "Epoch 1/20  Iteration 234/35720 Training loss: 3.0949 0.3606 sec/batch\n",
      "Epoch 1/20  Iteration 235/35720 Training loss: 3.0923 0.3593 sec/batch\n",
      "Epoch 1/20  Iteration 236/35720 Training loss: 3.0902 0.3580 sec/batch\n",
      "Epoch 1/20  Iteration 237/35720 Training loss: 3.0880 0.3604 sec/batch\n",
      "Epoch 1/20  Iteration 238/35720 Training loss: 3.0859 0.3599 sec/batch\n",
      "Epoch 1/20  Iteration 239/35720 Training loss: 3.0836 0.3573 sec/batch\n",
      "Epoch 1/20  Iteration 240/35720 Training loss: 3.0814 0.3574 sec/batch\n",
      "Epoch 1/20  Iteration 241/35720 Training loss: 3.0790 0.3574 sec/batch\n",
      "Epoch 1/20  Iteration 242/35720 Training loss: 3.0771 0.3573 sec/batch\n",
      "Epoch 1/20  Iteration 243/35720 Training loss: 3.0751 0.3605 sec/batch\n",
      "Epoch 1/20  Iteration 244/35720 Training loss: 3.0733 0.3570 sec/batch\n",
      "Epoch 1/20  Iteration 245/35720 Training loss: 3.0716 0.3591 sec/batch\n",
      "Epoch 1/20  Iteration 246/35720 Training loss: 3.0694 0.3575 sec/batch\n",
      "Epoch 1/20  Iteration 247/35720 Training loss: 3.0677 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 248/35720 Training loss: 3.0656 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 249/35720 Training loss: 3.0634 0.3622 sec/batch\n",
      "Epoch 1/20  Iteration 250/35720 Training loss: 3.0615 0.3585 sec/batch\n",
      "Epoch 1/20  Iteration 251/35720 Training loss: 3.0592 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 252/35720 Training loss: 3.0570 0.3614 sec/batch\n",
      "Epoch 1/20  Iteration 253/35720 Training loss: 3.0547 0.3581 sec/batch\n",
      "Epoch 1/20  Iteration 254/35720 Training loss: 3.0527 0.3572 sec/batch\n",
      "Epoch 1/20  Iteration 255/35720 Training loss: 3.0507 0.3594 sec/batch\n",
      "Epoch 1/20  Iteration 256/35720 Training loss: 3.0489 0.3589 sec/batch\n",
      "Epoch 1/20  Iteration 257/35720 Training loss: 3.0467 0.3611 sec/batch\n",
      "Epoch 1/20  Iteration 258/35720 Training loss: 3.0449 0.3612 sec/batch\n",
      "Epoch 1/20  Iteration 259/35720 Training loss: 3.0429 0.3611 sec/batch\n",
      "Epoch 1/20  Iteration 260/35720 Training loss: 3.0410 0.3570 sec/batch\n",
      "Epoch 1/20  Iteration 261/35720 Training loss: 3.0388 0.3573 sec/batch\n",
      "Epoch 1/20  Iteration 262/35720 Training loss: 3.0367 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 263/35720 Training loss: 3.0346 0.3573 sec/batch\n",
      "Epoch 1/20  Iteration 264/35720 Training loss: 3.0325 0.3571 sec/batch\n",
      "Epoch 1/20  Iteration 265/35720 Training loss: 3.0305 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 266/35720 Training loss: 3.0285 0.3613 sec/batch\n",
      "Epoch 1/20  Iteration 267/35720 Training loss: 3.0262 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 268/35720 Training loss: 3.0243 0.3573 sec/batch\n",
      "Epoch 1/20  Iteration 269/35720 Training loss: 3.0217 0.3585 sec/batch\n",
      "Epoch 1/20  Iteration 270/35720 Training loss: 3.0193 0.3575 sec/batch\n",
      "Epoch 1/20  Iteration 271/35720 Training loss: 3.0174 0.3581 sec/batch\n",
      "Epoch 1/20  Iteration 272/35720 Training loss: 3.0153 0.3628 sec/batch\n",
      "Epoch 1/20  Iteration 273/35720 Training loss: 3.0132 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 274/35720 Training loss: 3.0114 0.3678 sec/batch\n",
      "Epoch 1/20  Iteration 275/35720 Training loss: 3.0093 0.3631 sec/batch\n",
      "Epoch 1/20  Iteration 276/35720 Training loss: 3.0080 0.3651 sec/batch\n",
      "Epoch 1/20  Iteration 277/35720 Training loss: 3.0060 0.3643 sec/batch\n",
      "Epoch 1/20  Iteration 278/35720 Training loss: 3.0042 0.3602 sec/batch\n",
      "Epoch 1/20  Iteration 279/35720 Training loss: 3.0023 0.3624 sec/batch\n",
      "Epoch 1/20  Iteration 280/35720 Training loss: 3.0007 0.3605 sec/batch\n",
      "Epoch 1/20  Iteration 281/35720 Training loss: 2.9991 0.3593 sec/batch\n",
      "Epoch 1/20  Iteration 282/35720 Training loss: 2.9972 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 283/35720 Training loss: 2.9953 0.3641 sec/batch\n",
      "Epoch 1/20  Iteration 284/35720 Training loss: 2.9936 0.3675 sec/batch\n",
      "Epoch 1/20  Iteration 285/35720 Training loss: 2.9924 0.3585 sec/batch\n",
      "Epoch 1/20  Iteration 286/35720 Training loss: 2.9908 0.3605 sec/batch\n",
      "Epoch 1/20  Iteration 287/35720 Training loss: 2.9889 0.3631 sec/batch\n",
      "Epoch 1/20  Iteration 288/35720 Training loss: 2.9868 0.3610 sec/batch\n",
      "Epoch 1/20  Iteration 289/35720 Training loss: 2.9851 0.3622 sec/batch\n",
      "Epoch 1/20  Iteration 290/35720 Training loss: 2.9836 0.3622 sec/batch\n",
      "Epoch 1/20  Iteration 291/35720 Training loss: 2.9821 0.3582 sec/batch\n",
      "Epoch 1/20  Iteration 292/35720 Training loss: 2.9806 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 293/35720 Training loss: 2.9787 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 294/35720 Training loss: 2.9766 0.3587 sec/batch\n",
      "Epoch 1/20  Iteration 295/35720 Training loss: 2.9754 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 296/35720 Training loss: 2.9737 0.3590 sec/batch\n",
      "Epoch 1/20  Iteration 297/35720 Training loss: 2.9723 0.3611 sec/batch\n",
      "Epoch 1/20  Iteration 298/35720 Training loss: 2.9710 0.3608 sec/batch\n",
      "Epoch 1/20  Iteration 299/35720 Training loss: 2.9693 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 300/35720 Training loss: 2.9674 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 301/35720 Training loss: 2.9658 0.3579 sec/batch\n",
      "Epoch 1/20  Iteration 302/35720 Training loss: 2.9640 0.3600 sec/batch\n",
      "Epoch 1/20  Iteration 303/35720 Training loss: 2.9622 0.3587 sec/batch\n",
      "Epoch 1/20  Iteration 304/35720 Training loss: 2.9607 0.3636 sec/batch\n",
      "Epoch 1/20  Iteration 305/35720 Training loss: 2.9592 0.3643 sec/batch\n",
      "Epoch 1/20  Iteration 306/35720 Training loss: 2.9575 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 307/35720 Training loss: 2.9560 0.3621 sec/batch\n",
      "Epoch 1/20  Iteration 308/35720 Training loss: 2.9541 0.3574 sec/batch\n",
      "Epoch 1/20  Iteration 309/35720 Training loss: 2.9524 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 310/35720 Training loss: 2.9505 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 311/35720 Training loss: 2.9487 0.3579 sec/batch\n",
      "Epoch 1/20  Iteration 312/35720 Training loss: 2.9470 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 313/35720 Training loss: 2.9452 0.3684 sec/batch\n",
      "Epoch 1/20  Iteration 314/35720 Training loss: 2.9435 0.3605 sec/batch\n",
      "Epoch 1/20  Iteration 315/35720 Training loss: 2.9421 0.3589 sec/batch\n",
      "Epoch 1/20  Iteration 316/35720 Training loss: 2.9404 0.3582 sec/batch\n",
      "Epoch 1/20  Iteration 317/35720 Training loss: 2.9389 0.3581 sec/batch\n",
      "Epoch 1/20  Iteration 318/35720 Training loss: 2.9377 0.3612 sec/batch\n",
      "Epoch 1/20  Iteration 319/35720 Training loss: 2.9359 0.3580 sec/batch\n",
      "Epoch 1/20  Iteration 320/35720 Training loss: 2.9340 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 321/35720 Training loss: 2.9324 0.3620 sec/batch\n",
      "Epoch 1/20  Iteration 322/35720 Training loss: 2.9307 0.3582 sec/batch\n",
      "Epoch 1/20  Iteration 323/35720 Training loss: 2.9290 0.3572 sec/batch\n",
      "Epoch 1/20  Iteration 324/35720 Training loss: 2.9275 0.3573 sec/batch\n",
      "Epoch 1/20  Iteration 325/35720 Training loss: 2.9259 0.3618 sec/batch\n",
      "Epoch 1/20  Iteration 326/35720 Training loss: 2.9243 0.3613 sec/batch\n",
      "Epoch 1/20  Iteration 327/35720 Training loss: 2.9227 0.3571 sec/batch\n",
      "Epoch 1/20  Iteration 328/35720 Training loss: 2.9210 0.3576 sec/batch\n",
      "Epoch 1/20  Iteration 329/35720 Training loss: 2.9195 0.3624 sec/batch\n",
      "Epoch 1/20  Iteration 330/35720 Training loss: 2.9180 0.3590 sec/batch\n",
      "Epoch 1/20  Iteration 331/35720 Training loss: 2.9166 0.3610 sec/batch\n",
      "Epoch 1/20  Iteration 332/35720 Training loss: 2.9153 0.3582 sec/batch\n",
      "Epoch 1/20  Iteration 333/35720 Training loss: 2.9141 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 334/35720 Training loss: 2.9126 0.3580 sec/batch\n",
      "Epoch 1/20  Iteration 335/35720 Training loss: 2.9111 0.3626 sec/batch\n",
      "Epoch 1/20  Iteration 336/35720 Training loss: 2.9098 0.3580 sec/batch\n",
      "Epoch 1/20  Iteration 337/35720 Training loss: 2.9087 0.3627 sec/batch\n",
      "Epoch 1/20  Iteration 338/35720 Training loss: 2.9070 0.3587 sec/batch\n",
      "Epoch 1/20  Iteration 339/35720 Training loss: 2.9057 0.3590 sec/batch\n",
      "Epoch 1/20  Iteration 340/35720 Training loss: 2.9043 0.3585 sec/batch\n",
      "Epoch 1/20  Iteration 341/35720 Training loss: 2.9028 0.3590 sec/batch\n",
      "Epoch 1/20  Iteration 342/35720 Training loss: 2.9011 0.3628 sec/batch\n",
      "Epoch 1/20  Iteration 343/35720 Training loss: 2.8995 0.3575 sec/batch\n",
      "Epoch 1/20  Iteration 344/35720 Training loss: 2.8978 0.3585 sec/batch\n",
      "Epoch 1/20  Iteration 345/35720 Training loss: 2.8964 0.3605 sec/batch\n",
      "Epoch 1/20  Iteration 346/35720 Training loss: 2.8949 0.3677 sec/batch\n",
      "Epoch 1/20  Iteration 347/35720 Training loss: 2.8936 0.3572 sec/batch\n",
      "Epoch 1/20  Iteration 348/35720 Training loss: 2.8922 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 349/35720 Training loss: 2.8910 0.3608 sec/batch\n",
      "Epoch 1/20  Iteration 350/35720 Training loss: 2.8895 0.3596 sec/batch\n",
      "Epoch 1/20  Iteration 351/35720 Training loss: 2.8881 0.3581 sec/batch\n",
      "Epoch 1/20  Iteration 352/35720 Training loss: 2.8868 0.3615 sec/batch\n",
      "Epoch 1/20  Iteration 353/35720 Training loss: 2.8856 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 354/35720 Training loss: 2.8841 0.3576 sec/batch\n",
      "Epoch 1/20  Iteration 355/35720 Training loss: 2.8828 0.3637 sec/batch\n",
      "Epoch 1/20  Iteration 356/35720 Training loss: 2.8813 0.3631 sec/batch\n",
      "Epoch 1/20  Iteration 357/35720 Training loss: 2.8799 0.3605 sec/batch\n",
      "Epoch 1/20  Iteration 358/35720 Training loss: 2.8782 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 359/35720 Training loss: 2.8767 0.3609 sec/batch\n",
      "Epoch 1/20  Iteration 360/35720 Training loss: 2.8754 0.3579 sec/batch\n",
      "Epoch 1/20  Iteration 361/35720 Training loss: 2.8739 0.3617 sec/batch\n",
      "Epoch 1/20  Iteration 362/35720 Training loss: 2.8726 0.3657 sec/batch\n",
      "Epoch 1/20  Iteration 363/35720 Training loss: 2.8711 0.3586 sec/batch\n",
      "Epoch 1/20  Iteration 364/35720 Training loss: 2.8696 0.3585 sec/batch\n",
      "Epoch 1/20  Iteration 365/35720 Training loss: 2.8682 0.3587 sec/batch\n",
      "Epoch 1/20  Iteration 366/35720 Training loss: 2.8668 0.3618 sec/batch\n",
      "Epoch 1/20  Iteration 367/35720 Training loss: 2.8654 0.3619 sec/batch\n",
      "Epoch 1/20  Iteration 368/35720 Training loss: 2.8641 0.3579 sec/batch\n",
      "Epoch 1/20  Iteration 369/35720 Training loss: 2.8628 0.3582 sec/batch\n",
      "Epoch 1/20  Iteration 370/35720 Training loss: 2.8616 0.3584 sec/batch\n",
      "Epoch 1/20  Iteration 371/35720 Training loss: 2.8600 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 372/35720 Training loss: 2.8586 0.3586 sec/batch\n",
      "Epoch 1/20  Iteration 373/35720 Training loss: 2.8574 0.3580 sec/batch\n",
      "Epoch 1/20  Iteration 374/35720 Training loss: 2.8562 0.3625 sec/batch\n",
      "Epoch 1/20  Iteration 375/35720 Training loss: 2.8548 0.3596 sec/batch\n",
      "Epoch 1/20  Iteration 376/35720 Training loss: 2.8537 0.3608 sec/batch\n",
      "Epoch 1/20  Iteration 377/35720 Training loss: 2.8524 0.3582 sec/batch\n",
      "Epoch 1/20  Iteration 378/35720 Training loss: 2.8508 0.3681 sec/batch\n",
      "Epoch 1/20  Iteration 379/35720 Training loss: 2.8496 0.3601 sec/batch\n",
      "Epoch 1/20  Iteration 380/35720 Training loss: 2.8484 0.3679 sec/batch\n",
      "Epoch 1/20  Iteration 381/35720 Training loss: 2.8470 0.3599 sec/batch\n",
      "Epoch 1/20  Iteration 382/35720 Training loss: 2.8458 0.3579 sec/batch\n",
      "Epoch 1/20  Iteration 383/35720 Training loss: 2.8448 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 384/35720 Training loss: 2.8435 0.3604 sec/batch\n",
      "Epoch 1/20  Iteration 385/35720 Training loss: 2.8423 0.3674 sec/batch\n",
      "Epoch 1/20  Iteration 386/35720 Training loss: 2.8409 0.3628 sec/batch\n",
      "Epoch 1/20  Iteration 387/35720 Training loss: 2.8400 0.3584 sec/batch\n",
      "Epoch 1/20  Iteration 388/35720 Training loss: 2.8387 0.3609 sec/batch\n",
      "Epoch 1/20  Iteration 389/35720 Training loss: 2.8376 0.3580 sec/batch\n",
      "Epoch 1/20  Iteration 390/35720 Training loss: 2.8365 0.3611 sec/batch\n",
      "Epoch 1/20  Iteration 391/35720 Training loss: 2.8351 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 392/35720 Training loss: 2.8338 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 393/35720 Training loss: 2.8327 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 394/35720 Training loss: 2.8314 0.3729 sec/batch\n",
      "Epoch 1/20  Iteration 395/35720 Training loss: 2.8301 0.3631 sec/batch\n",
      "Epoch 1/20  Iteration 396/35720 Training loss: 2.8286 0.3702 sec/batch\n",
      "Epoch 1/20  Iteration 397/35720 Training loss: 2.8273 0.3640 sec/batch\n",
      "Epoch 1/20  Iteration 398/35720 Training loss: 2.8259 0.3649 sec/batch\n",
      "Epoch 1/20  Iteration 399/35720 Training loss: 2.8246 0.3676 sec/batch\n",
      "Epoch 1/20  Iteration 400/35720 Training loss: 2.8233 0.3624 sec/batch\n",
      "Validation loss: 2.32223 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 401/35720 Training loss: 2.8219 0.3638 sec/batch\n",
      "Epoch 1/20  Iteration 402/35720 Training loss: 2.8207 0.3618 sec/batch\n",
      "Epoch 1/20  Iteration 403/35720 Training loss: 2.8195 0.3571 sec/batch\n",
      "Epoch 1/20  Iteration 404/35720 Training loss: 2.8181 0.3601 sec/batch\n",
      "Epoch 1/20  Iteration 405/35720 Training loss: 2.8167 0.3615 sec/batch\n",
      "Epoch 1/20  Iteration 406/35720 Training loss: 2.8155 0.3573 sec/batch\n",
      "Epoch 1/20  Iteration 407/35720 Training loss: 2.8141 0.3588 sec/batch\n",
      "Epoch 1/20  Iteration 408/35720 Training loss: 2.8127 0.3595 sec/batch\n",
      "Epoch 1/20  Iteration 409/35720 Training loss: 2.8113 0.3654 sec/batch\n",
      "Epoch 1/20  Iteration 410/35720 Training loss: 2.8102 0.3672 sec/batch\n",
      "Epoch 1/20  Iteration 411/35720 Training loss: 2.8089 0.3592 sec/batch\n",
      "Epoch 1/20  Iteration 412/35720 Training loss: 2.8075 0.3597 sec/batch\n",
      "Epoch 1/20  Iteration 413/35720 Training loss: 2.8063 0.3574 sec/batch\n",
      "Epoch 1/20  Iteration 414/35720 Training loss: 2.8051 0.3629 sec/batch\n",
      "Epoch 1/20  Iteration 415/35720 Training loss: 2.8038 0.3580 sec/batch\n",
      "Epoch 1/20  Iteration 416/35720 Training loss: 2.8026 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 417/35720 Training loss: 2.8014 0.3575 sec/batch\n",
      "Epoch 1/20  Iteration 418/35720 Training loss: 2.8003 0.3635 sec/batch\n",
      "Epoch 1/20  Iteration 419/35720 Training loss: 2.7990 0.3626 sec/batch\n",
      "Epoch 1/20  Iteration 420/35720 Training loss: 2.7979 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 421/35720 Training loss: 2.7968 0.3597 sec/batch\n",
      "Epoch 1/20  Iteration 422/35720 Training loss: 2.7956 0.3592 sec/batch\n",
      "Epoch 1/20  Iteration 423/35720 Training loss: 2.7941 0.3572 sec/batch\n",
      "Epoch 1/20  Iteration 424/35720 Training loss: 2.7930 0.3615 sec/batch\n",
      "Epoch 1/20  Iteration 425/35720 Training loss: 2.7918 0.3627 sec/batch\n",
      "Epoch 1/20  Iteration 426/35720 Training loss: 2.7907 0.3618 sec/batch\n",
      "Epoch 1/20  Iteration 427/35720 Training loss: 2.7895 0.3572 sec/batch\n",
      "Epoch 1/20  Iteration 428/35720 Training loss: 2.7884 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 429/35720 Training loss: 2.7873 0.3575 sec/batch\n",
      "Epoch 1/20  Iteration 430/35720 Training loss: 2.7864 0.3621 sec/batch\n",
      "Epoch 1/20  Iteration 431/35720 Training loss: 2.7852 0.3627 sec/batch\n",
      "Epoch 1/20  Iteration 432/35720 Training loss: 2.7840 0.3582 sec/batch\n",
      "Epoch 1/20  Iteration 433/35720 Training loss: 2.7831 0.3591 sec/batch\n",
      "Epoch 1/20  Iteration 434/35720 Training loss: 2.7820 0.3586 sec/batch\n",
      "Epoch 1/20  Iteration 435/35720 Training loss: 2.7810 0.3608 sec/batch\n",
      "Epoch 1/20  Iteration 436/35720 Training loss: 2.7798 0.3591 sec/batch\n",
      "Epoch 1/20  Iteration 437/35720 Training loss: 2.7786 0.3591 sec/batch\n",
      "Epoch 1/20  Iteration 438/35720 Training loss: 2.7775 0.3625 sec/batch\n",
      "Epoch 1/20  Iteration 439/35720 Training loss: 2.7764 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 440/35720 Training loss: 2.7752 0.3597 sec/batch\n",
      "Epoch 1/20  Iteration 441/35720 Training loss: 2.7742 0.3599 sec/batch\n",
      "Epoch 1/20  Iteration 442/35720 Training loss: 2.7731 0.3573 sec/batch\n",
      "Epoch 1/20  Iteration 443/35720 Training loss: 2.7718 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 444/35720 Training loss: 2.7706 0.3630 sec/batch\n",
      "Epoch 1/20  Iteration 445/35720 Training loss: 2.7695 0.3589 sec/batch\n",
      "Epoch 1/20  Iteration 446/35720 Training loss: 2.7684 0.3602 sec/batch\n",
      "Epoch 1/20  Iteration 447/35720 Training loss: 2.7673 0.3635 sec/batch\n",
      "Epoch 1/20  Iteration 448/35720 Training loss: 2.7660 0.3629 sec/batch\n",
      "Epoch 1/20  Iteration 449/35720 Training loss: 2.7649 0.3596 sec/batch\n",
      "Epoch 1/20  Iteration 450/35720 Training loss: 2.7638 0.3574 sec/batch\n",
      "Epoch 1/20  Iteration 451/35720 Training loss: 2.7626 0.3596 sec/batch\n",
      "Epoch 1/20  Iteration 452/35720 Training loss: 2.7615 0.3639 sec/batch\n",
      "Epoch 1/20  Iteration 453/35720 Training loss: 2.7604 0.3596 sec/batch\n",
      "Epoch 1/20  Iteration 454/35720 Training loss: 2.7594 0.3710 sec/batch\n",
      "Epoch 1/20  Iteration 455/35720 Training loss: 2.7584 0.3667 sec/batch\n",
      "Epoch 1/20  Iteration 456/35720 Training loss: 2.7573 0.3623 sec/batch\n",
      "Epoch 1/20  Iteration 457/35720 Training loss: 2.7562 0.3602 sec/batch\n",
      "Epoch 1/20  Iteration 458/35720 Training loss: 2.7551 0.3575 sec/batch\n",
      "Epoch 1/20  Iteration 459/35720 Training loss: 2.7540 0.3625 sec/batch\n",
      "Epoch 1/20  Iteration 460/35720 Training loss: 2.7528 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 461/35720 Training loss: 2.7517 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 462/35720 Training loss: 2.7505 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 463/35720 Training loss: 2.7493 0.3583 sec/batch\n",
      "Epoch 1/20  Iteration 464/35720 Training loss: 2.7481 0.3633 sec/batch\n",
      "Epoch 1/20  Iteration 465/35720 Training loss: 2.7469 0.3573 sec/batch\n",
      "Epoch 1/20  Iteration 466/35720 Training loss: 2.7459 0.3574 sec/batch\n",
      "Epoch 1/20  Iteration 467/35720 Training loss: 2.7447 0.3614 sec/batch\n",
      "Epoch 1/20  Iteration 468/35720 Training loss: 2.7436 0.3618 sec/batch\n",
      "Epoch 1/20  Iteration 469/35720 Training loss: 2.7425 0.3617 sec/batch\n",
      "Epoch 1/20  Iteration 470/35720 Training loss: 2.7413 0.3611 sec/batch\n",
      "Epoch 1/20  Iteration 471/35720 Training loss: 2.7401 0.3608 sec/batch\n",
      "Epoch 1/20  Iteration 472/35720 Training loss: 2.7390 0.3571 sec/batch\n",
      "Epoch 1/20  Iteration 473/35720 Training loss: 2.7379 0.3622 sec/batch\n",
      "Epoch 1/20  Iteration 474/35720 Training loss: 2.7367 0.3613 sec/batch\n",
      "Epoch 1/20  Iteration 475/35720 Training loss: 2.7356 0.3608 sec/batch\n",
      "Epoch 1/20  Iteration 476/35720 Training loss: 2.7346 0.3625 sec/batch\n",
      "Epoch 1/20  Iteration 477/35720 Training loss: 2.7335 0.3620 sec/batch\n",
      "Epoch 1/20  Iteration 478/35720 Training loss: 2.7325 0.3576 sec/batch\n",
      "Epoch 1/20  Iteration 479/35720 Training loss: 2.7315 0.3581 sec/batch\n",
      "Epoch 1/20  Iteration 480/35720 Training loss: 2.7305 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 481/35720 Training loss: 2.7292 0.3612 sec/batch\n",
      "Epoch 1/20  Iteration 482/35720 Training loss: 2.7282 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 483/35720 Training loss: 2.7273 0.3591 sec/batch\n",
      "Epoch 1/20  Iteration 484/35720 Training loss: 2.7262 0.3586 sec/batch\n",
      "Epoch 1/20  Iteration 485/35720 Training loss: 2.7252 0.3708 sec/batch\n",
      "Epoch 1/20  Iteration 486/35720 Training loss: 2.7242 0.3769 sec/batch\n",
      "Epoch 1/20  Iteration 487/35720 Training loss: 2.7231 0.3710 sec/batch\n",
      "Epoch 1/20  Iteration 488/35720 Training loss: 2.7221 0.3662 sec/batch\n",
      "Epoch 1/20  Iteration 489/35720 Training loss: 2.7213 0.3574 sec/batch\n",
      "Epoch 1/20  Iteration 490/35720 Training loss: 2.7203 0.3579 sec/batch\n",
      "Epoch 1/20  Iteration 491/35720 Training loss: 2.7193 0.3584 sec/batch\n",
      "Epoch 1/20  Iteration 492/35720 Training loss: 2.7182 0.3586 sec/batch\n",
      "Epoch 1/20  Iteration 493/35720 Training loss: 2.7170 0.3590 sec/batch\n",
      "Epoch 1/20  Iteration 494/35720 Training loss: 2.7159 0.3609 sec/batch\n",
      "Epoch 1/20  Iteration 495/35720 Training loss: 2.7149 0.3630 sec/batch\n",
      "Epoch 1/20  Iteration 496/35720 Training loss: 2.7138 0.3627 sec/batch\n",
      "Epoch 1/20  Iteration 497/35720 Training loss: 2.7130 0.3635 sec/batch\n",
      "Epoch 1/20  Iteration 498/35720 Training loss: 2.7120 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 499/35720 Training loss: 2.7110 0.3619 sec/batch\n",
      "Epoch 1/20  Iteration 500/35720 Training loss: 2.7099 0.3603 sec/batch\n",
      "Epoch 1/20  Iteration 501/35720 Training loss: 2.7087 0.3612 sec/batch\n",
      "Epoch 1/20  Iteration 502/35720 Training loss: 2.7077 0.3574 sec/batch\n",
      "Epoch 1/20  Iteration 503/35720 Training loss: 2.7067 0.3577 sec/batch\n",
      "Epoch 1/20  Iteration 504/35720 Training loss: 2.7058 0.3572 sec/batch\n",
      "Epoch 1/20  Iteration 505/35720 Training loss: 2.7049 0.3597 sec/batch\n",
      "Epoch 1/20  Iteration 506/35720 Training loss: 2.7040 0.3603 sec/batch\n",
      "Epoch 1/20  Iteration 507/35720 Training loss: 2.7031 0.3573 sec/batch\n",
      "Epoch 1/20  Iteration 508/35720 Training loss: 2.7020 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 509/35720 Training loss: 2.7011 0.3630 sec/batch\n",
      "Epoch 1/20  Iteration 510/35720 Training loss: 2.7001 0.3576 sec/batch\n",
      "Epoch 1/20  Iteration 511/35720 Training loss: 2.6992 0.3623 sec/batch\n",
      "Epoch 1/20  Iteration 512/35720 Training loss: 2.6982 0.3578 sec/batch\n",
      "Epoch 1/20  Iteration 513/35720 Training loss: 2.6972 0.3618 sec/batch\n",
      "Epoch 1/20  Iteration 514/35720 Training loss: 2.6962 0.3574 sec/batch\n",
      "Epoch 1/20  Iteration 515/35720 Training loss: 2.6952 0.3611 sec/batch\n",
      "Epoch 1/20  Iteration 516/35720 Training loss: 2.6940 0.3575 sec/batch\n",
      "Epoch 1/20  Iteration 517/35720 Training loss: 2.6930 0.3579 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/____.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
